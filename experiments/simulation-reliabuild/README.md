# Descriptive Expression + Scheduler Simulation

> Algorithm: direct match with reliabuild metadata

- [publication](https://www.osti.gov/biblio/2223030)
- [build spec generator](https://github.com/buildsi/spack-buildspace-exploration)

We started with a base simulation in [simulation-lammps](../simulation-lammps) and this experiment aims to take a new angle - bringing in the dimension of being able to match a range of versions for compatibility. Specifically, we have spack builds that we know were successful with specific versions of software, and we can:

1. prepare clusters with randomly selected versions
1. associate each successful build dependency version with a range (making the assumption the range will allow to build)
1. ask rainbow to assign builds based on matching version ranges (the cluster is in the range the build needs for one or more dependencies)

For the above, we are interested to know:

1. How many builds would be successful based on having all dependencies in the matched ranges
1. The change in matching clusters as we add compatibility metadata (more dependency version constaints)
1. The marginal points (number of version ranges that were right, even if the build might fail) for each.
1. Patterns of the above across cases of providing no compatibility metadata, metadata for one subsystem (package dependency) and then increasing numbers of subsystems
1. The incremental addition (number of subsystems) that deemed rainbow could not match the job anywhere.

This obviously is an imperfect experiment - and indeed I worked days on it and am still not happy. The reasons are because the number of clusters will influence the result, along with the choice I made about how large each version range it. It ultimately turned into a game of matching version ranges for a set of known building packages, and assuming that if we broke that range it would not be a successful build (not necessarily true, but very possible knowing spack). Despite that, I think the addition to rainbow to support matching a range is useful, and this experiment can help to start thinking for better experiments. We likely need a similar dataset to this one, but with metadata more true to something not building (e.g., arch, MPI, that sort of thing).

I kept very detailed results so minimally the code might be refactored to do something better.

### Experiment

An overview of the experiment is the following. See [details](#details) for more of that.

1. For the real dataset (spack specs) filter down to those that were successful. We know they build.
1. The package dependency versions are the features. This means rainbow needed an algorithm addition to match a version range [PR](https://github.com/converged-computing/rainbow/pull/21).
1. Create 100 cluster environments that have a randomly selected version (from the set in the experiment)
1. One level of compatibility metadata is a dependency version. Submit jobs to rainbow, first with no metadata, and then adding version *ranges* for specific packages. E.g., "Does this build environment have a version that is within this range?" 
1. When we finish submitting all jobspecs (spack build specs) we have a set assigned to every cluster.
1. Calculate a score (versions that correctly overlap over total)

#### 1. Rainbow

First, let's run rainbow. We will keep this really simple - running locally, and that way you could, for example, add in a new algorithm and re-deploy the cluster with just a control-C and `make server`. 

```bash
git clone https://github.com/converged-computing/rainbow
cd rainbow
go mod vendor

# See what you can do
go run cmd/server/server.go --help

# We can use all the defaults, and set a global token. Run it:
go run cmd/server/server.go --global-token rainbow --match-algorithm range
```
```console
2024/03/20 00:27:55 creating üåàÔ∏è server...
2024/03/20 00:27:55 üß©Ô∏è selection algorithm: random
2024/03/20 00:27:55 üß©Ô∏è graph database: memory
2024/03/20 00:27:55 ‚ú®Ô∏è creating rainbow.db...
2024/03/20 00:27:55    rainbow.db file created
2024/03/20 00:27:55    üèìÔ∏è creating tables...
2024/03/20 00:27:55    üèìÔ∏è tables created
2024/03/20 00:27:55 ‚ö†Ô∏è WARNING: global-token is set, use with caution.
2024/03/20 00:27:55 starting scheduler server: rainbow v0.1.1-draft
2024/03/20 00:27:55 üß†Ô∏è Registering memory graph database...
2024/03/20 00:27:55 server listening: [::]:50051
```

#### 2. Details

Since we have everything locally, we can actually do the entire thing in Python! We will (in one script, I know):

For each cluster:

1. register 100 randomly generated clusters, each with varying ranges of dependency versions (subsystems)
1. register ~219 subsystems (a package version) across 100 clusters
1. run the experiment with ~10K jobspecs without any subsystem metadata
1. run the expemiments adding each subsystem in isolation
1. for each subsystem (randomly sorted):
 - submit each jobspec with added subsystem metadata, adding subsystem metadata until no match (then stopping)
 - for each cluster, get assigned jobs:
  - give a point for each package build dependency version in the right range
  - keep track of overall points (correct version ranges) vs. binary outcomes (anticipated build success / failure)
  - keep track of unmatched jobs, etc.
  - save scores across cluster

I am calling them "anticipated" build success and failure because we cannot be absolutely sure moving a version outside of a desired range would break it, but likely it would. It's also likely the case that I made the ranges too big, and in fact even a small deviance would have broken the build. 

```bash
python run_experiments.py
```

## Results

Parse the results:

```bash
python plot-results.py
```
